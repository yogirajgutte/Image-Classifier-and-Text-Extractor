{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import required modules and libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.applications.vgg19 import VGG19\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.optimizers import RMSprop\n",
    "from tensorflow.keras.callbacks import ReduceLROnPlateau\n",
    "from tensorflow.keras.layers import Flatten, Dense, BatchNormalization, Activation,Dropout\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "import PIL\n",
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '-1'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load a pre-trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "pre_trained_model = VGG19(input_shape = (150,150,3),\n",
    "                          include_top = False,\n",
    "                          weights='imagenet',\n",
    "                          classes = 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Freeze the convolutional layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "for layer in pre_trained_model.layers:\n",
    "    layer.trainable = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build new model based on the pre-trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Adding the final layers to the above base models where the actual classification is done in the dense layers\n",
    "model = Sequential()\n",
    "model.add(pre_trained_model) \n",
    "model.add(Flatten()) \n",
    "model.add(Dense(1024,activation=('relu'),input_dim=512))\n",
    "model.add(Dense(512,activation=('relu'))) \n",
    "model.add(Dense(256,activation=('relu'))) \n",
    "model.add(Dropout(.3))\n",
    "model.add(Dense(128,activation=('relu')))\n",
    "model.add(Dropout(.2))\n",
    "model.add(Dense(10,activation=('softmax')))\n",
    "\n",
    "\n",
    "# from tensorflow.keras.optimizers import RMSprop\n",
    "# from tensorflow.keras import layers, Model\n",
    "\n",
    "# x = layers.Flatten()(pre_trained_model.output)\n",
    "\n",
    "# x = layers.Dense(1024, activation ='relu')(x)\n",
    "# x = layers.Dropout(0.2)(x)\n",
    "# x = layers.Dense(10,activation='sigmoid')(x)\n",
    "\n",
    "# model = Model(pre_trained_model.input, x)\n",
    "\n",
    "# model.compile(optimizer = RMSprop(lr=0.0001),\n",
    "#               loss = 'binary_crossentropy',\n",
    "#               metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "vgg19 (Functional)           (None, 4, 4, 512)         20024384  \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 8192)              0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 1024)              8389632   \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 512)               524800    \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 256)               131328    \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 128)               32896     \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 10)                1290      \n",
      "=================================================================\n",
      "Total params: 29,104,330\n",
      "Trainable params: 9,079,946\n",
      "Non-trainable params: 20,024,384\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Defining the hyperparameters\n",
    "# learn_rate=.001\n",
    "# sgd=SGD(lr=learn_rate,momentum=.9,nesterov=False)\n",
    "\n",
    "#Compiling the VGG19 model\n",
    "model.compile(optimizer=RMSprop(lr=0.001),\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare dataset for processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 55745 images belonging to 10 classes.\n"
     ]
    }
   ],
   "source": [
    "train_datagen = ImageDataGenerator(\n",
    "    rescale=1./255,\n",
    "    shear_range = 0.2,\n",
    "    zoom_range=0.2,\n",
    "    horizontal_flip =True)\n",
    "\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "    \"D:/Dataset/train\",\n",
    "    target_size=(150,150),\n",
    "    batch_size = 32,\n",
    "    class_mode='categorical')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 18459 images belonging to 10 classes.\n"
     ]
    }
   ],
   "source": [
    "val_datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "val_generator = val_datagen.flow_from_directory(\n",
    "    \"D:/Dataset/validate\",\n",
    "    target_size=(150, 150),\n",
    "    batch_size =32,\n",
    "    class_mode='categorical')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train the new model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "100/100 [==============================] - 1784s 18s/step - loss: 2.3441 - accuracy: 0.4494 - val_loss: 0.5538 - val_accuracy: 0.8207\n",
      "Epoch 2/20\n",
      "100/100 [==============================] - 1569s 16s/step - loss: 0.4493 - accuracy: 0.8655 - val_loss: 0.3095 - val_accuracy: 0.9127\n",
      "Epoch 3/20\n",
      "100/100 [==============================] - 1546s 16s/step - loss: 0.4005 - accuracy: 0.8895 - val_loss: 0.2443 - val_accuracy: 0.9347\n",
      "Epoch 4/20\n",
      "100/100 [==============================] - 1563s 16s/step - loss: 0.3071 - accuracy: 0.9162 - val_loss: 0.3365 - val_accuracy: 0.9114\n",
      "Epoch 5/20\n",
      "100/100 [==============================] - 1550s 16s/step - loss: 0.2679 - accuracy: 0.9302 - val_loss: 0.3052 - val_accuracy: 0.9268\n",
      "Epoch 6/20\n",
      "100/100 [==============================] - 1554s 16s/step - loss: 0.3407 - accuracy: 0.9169 - val_loss: 0.3320 - val_accuracy: 0.9231\n",
      "Epoch 7/20\n",
      "100/100 [==============================] - 1555s 16s/step - loss: 0.2785 - accuracy: 0.9316 - val_loss: 0.2405 - val_accuracy: 0.9381\n",
      "Epoch 8/20\n",
      "100/100 [==============================] - 1553s 16s/step - loss: 0.2486 - accuracy: 0.9266 - val_loss: 0.2004 - val_accuracy: 0.9568\n",
      "Epoch 9/20\n",
      "100/100 [==============================] - 1557s 16s/step - loss: 0.2416 - accuracy: 0.9466 - val_loss: 0.8353 - val_accuracy: 0.8762\n",
      "Epoch 10/20\n",
      "100/100 [==============================] - 1557s 16s/step - loss: 0.2117 - accuracy: 0.9439 - val_loss: 0.3651 - val_accuracy: 0.9065\n",
      "Epoch 11/20\n",
      "100/100 [==============================] - 1557s 16s/step - loss: 0.2383 - accuracy: 0.9366 - val_loss: 0.3517 - val_accuracy: 0.9248\n",
      "Epoch 12/20\n",
      "100/100 [==============================] - 1556s 16s/step - loss: 0.2550 - accuracy: 0.9460 - val_loss: 0.2489 - val_accuracy: 0.9451\n",
      "Epoch 13/20\n",
      "100/100 [==============================] - 1554s 16s/step - loss: 0.2496 - accuracy: 0.9345 - val_loss: 0.2469 - val_accuracy: 0.9406\n",
      "Epoch 14/20\n",
      "100/100 [==============================] - 1547s 16s/step - loss: 0.2391 - accuracy: 0.9420 - val_loss: 0.2380 - val_accuracy: 0.9500\n",
      "Epoch 15/20\n",
      "100/100 [==============================] - 1553s 16s/step - loss: 0.2572 - accuracy: 0.9406 - val_loss: 0.2172 - val_accuracy: 0.9460\n",
      "Epoch 16/20\n",
      "100/100 [==============================] - 1554s 16s/step - loss: 0.2687 - accuracy: 0.9491 - val_loss: 0.2100 - val_accuracy: 0.9570\n",
      "Epoch 17/20\n",
      "100/100 [==============================] - 1549s 16s/step - loss: 0.2911 - accuracy: 0.9317 - val_loss: 0.3201 - val_accuracy: 0.9323\n",
      "Epoch 18/20\n",
      "100/100 [==============================] - 1551s 16s/step - loss: 0.2749 - accuracy: 0.9315 - val_loss: 0.2280 - val_accuracy: 0.9601\n",
      "Epoch 19/20\n",
      "100/100 [==============================] - 1549s 16s/step - loss: 0.2912 - accuracy: 0.9282 - val_loss: 0.3221 - val_accuracy: 0.9284\n",
      "Epoch 20/20\n",
      "100/100 [==============================] - 1551s 16s/step - loss: 0.2399 - accuracy: 0.9403 - val_loss: 0.7305 - val_accuracy: 0.9139\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x25ab35b7370>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(train_generator,\n",
    "          steps_per_epoch=100,\n",
    "          epochs=20,\n",
    "          validation_data=val_generator,\n",
    "          validation_steps=18459/32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./VGG19_tf\\assets\n"
     ]
    }
   ],
   "source": [
    "model.save(\"./VGG19_tf\", save_format = 'tf')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
